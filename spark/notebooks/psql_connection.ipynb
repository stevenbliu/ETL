{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+--------+-------------------+\n",
      "|transaction_id|user_id|amount|    type|          timestamp|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "|    1704169733|      9|361.83|transfer|2024-01-01 20:28:53|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#.master(\"spark://spark-master:7077\") \\\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark with PostgreSQL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://psql-postgres-1:5432/mydatabase\"\n",
    "connection_properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mysecretpassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Load a table into Spark\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"transactions\", properties=connection_properties)\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path you want to check\n",
    "path = \"/opt/spark/jars/postgresql-42.7.4.jar\"\n",
    "\n",
    "# Check if the path exists\n",
    "if os.path.exists(path):\n",
    "    print(\"Path exists!\")\n",
    "else:\n",
    "    print(\"Path does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "(1704169733, 9, Decimal('361.83'), 'transfer', datetime.datetime(2024, 1, 1, 20, 28, 53))\n",
      "(1706848193, 77, Decimal('55.28'), 'transfer', datetime.datetime(2024, 2, 1, 20, 29, 53))\n",
      "(1709353853, 69, Decimal('82.96'), 'purchase', datetime.datetime(2024, 3, 1, 20, 30, 53))\n",
      "(1712028713, 88, Decimal('36.10'), 'transfer', datetime.datetime(2024, 4, 1, 20, 31, 53))\n",
      "(1714620773, 22, Decimal('94.44'), 'transfer', datetime.datetime(2024, 5, 1, 20, 32, 53))\n",
      "(1717299233, 73, Decimal('408.42'), 'purchase', datetime.datetime(2024, 6, 1, 20, 33, 53))\n",
      "(1719891293, 64, Decimal('954.82'), 'transfer', datetime.datetime(2024, 7, 1, 20, 34, 53))\n",
      "(1722569753, 59, Decimal('410.94'), 'purchase', datetime.datetime(2024, 8, 1, 20, 35, 53))\n",
      "(1725248213, 16, Decimal('797.11'), 'purchase', datetime.datetime(2024, 9, 1, 20, 36, 53))\n",
      "(1727840273, 42, Decimal('802.94'), 'purchase', datetime.datetime(2024, 10, 1, 20, 37, 53))\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host=\"psql-postgres-1\",\n",
    "    port=5432,\n",
    "    user=\"myuser\",\n",
    "    password=\"mysecretpassword\",\n",
    "    dbname=\"mydatabase\"\n",
    ")\n",
    "print(\"Connection successful!\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM transactions;\n",
    "\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Print each row\n",
    "for i, row in enumerate(rows):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(row)\n",
    "    \n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 29|\n",
      "|  Bob| 35|\n",
      "|Cathy| 45|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|  Bob| 35|\n",
      "|Cathy| 45|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "if 'spark' in globals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(\"Alice\", 29), (\"Bob\", 35), (\"Cathy\", 45)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Perform a simple transformation: filter by age > 30\n",
    "df_filtered = df.filter(df[\"Age\"] > 30)\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "df_filtered.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spark' in globals():\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpyspark\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SimpleApp\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
