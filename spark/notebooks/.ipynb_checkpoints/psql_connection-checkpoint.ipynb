{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import utils as utils\n",
    "import spark_utils as spark_utils\n",
    "#.master(\"spark://spark-master:7077\") \\\n",
    "\n",
    "problem_data_path = 'problematic_transactions.csv'\n",
    "\n",
    "utils.verify_path(problem_data_path)\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+--------+-------------------+\n",
      "|transaction_id|user_id|amount|    type|          timestamp|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "|    1704169733|      9|361.83|transfer|2024-01-01 20:28:53|\n",
      "|    1706848193|     77| 55.28|transfer|2024-02-01 20:29:53|\n",
      "|    1709353853|     69| 82.96|purchase|2024-03-01 20:30:53|\n",
      "|    1712028713|     88| 36.10|transfer|2024-04-01 20:31:53|\n",
      "|    1714620773|     22| 94.44|transfer|2024-05-01 20:32:53|\n",
      "|    1717299233|     73|408.42|purchase|2024-06-01 20:33:53|\n",
      "|    1719891293|     64|954.82|transfer|2024-07-01 20:34:53|\n",
      "|    1722569753|     59|410.94|purchase|2024-08-01 20:35:53|\n",
      "|    1725248213|     16|797.11|purchase|2024-09-01 20:36:53|\n",
      "|    1727840273|     42|802.94|purchase|2024-10-01 20:37:53|\n",
      "|    1730518733|     43| 35.93|transfer|2024-11-01 20:38:53|\n",
      "|    1733114393|     69|555.90|  refund|2024-12-01 20:39:53|\n",
      "|    1704170453|     55| 15.16|transfer|2024-01-01 20:40:53|\n",
      "|    1706848913|     24|489.46|transfer|2024-02-01 20:41:53|\n",
      "|    1709354573|      6| 87.45|  refund|2024-03-01 20:42:53|\n",
      "|    1712029433|      2|646.71|purchase|2024-04-01 20:43:53|\n",
      "|    1714621493|     27|424.29|transfer|2024-05-01 20:44:53|\n",
      "|    1717299953|     56|858.77|  refund|2024-06-01 20:45:53|\n",
      "|    1719892013|     80| 24.86|purchase|2024-07-01 20:46:53|\n",
      "|    1722570473|      3|217.34|  refund|2024-08-01 20:47:53|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Path exists!\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "|transaction_id|user_id|        amount|    type|           timestamp|\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "|    1704314682|     45|960.52 dollars|  refund|2024-01-03T12:44:42Z|\n",
      "|    1706993142|     70|434.41 dollars|purchase|2024-02-03T12:45:42Z|\n",
      "|    1709498802|     80|856.68 dollars|  refund|2024-03-03T12:46:42Z|\n",
      "|    1712173662|     10|          NULL|transfer|2024-04-03T12:47:42Z|\n",
      "|    1714765722|     88|        108.19|transfer|2024-05-03T12:44:42Z|\n",
      "|    1717444182|     45|          NULL|  refund|2024-06-03T12:49:42Z|\n",
      "|    1720036242|     24|        950.52|purchase|2024-07-03T12:50:42Z|\n",
      "|    1720036242|     24|        950.52|purchase|2024-07-03T12:50:42Z|\n",
      "|    1722714702|     51|        479.82|transfer|2024/08-03T12:51:42Z|\n",
      "|    1725393162|     93|        671.08|  refund|2024-09-03T11:39:42Z|\n",
      "|    1727985222|     60|505.53 dollars|purchase|2024-10-03T12:53:42Z|\n",
      "|    1730667282|     18|145.33 dollars|  refund|2024-11-03T12:54:42Z|\n",
      "|    1733259342|     71|        575.49|transfer|2024-12-03T11:27:42Z|\n",
      "|    1704315402|     44|        954.79|purchase|2024-01-03T12:18:42Z|\n",
      "|    1706993862|     37|          NULL|transfer|2024-02-03T12:57:42Z|\n",
      "|    1709499522|     44|         388.2|purchase|2024-03-03T12:58:42Z|\n",
      "|    1709499522|     44|         388.2|purchase|2024-03-03T12:58:42Z|\n",
      "|    1712174382|     97|        715.28|purchase|2024-04-03T12:59:42Z|\n",
      "|    1712174382|     97|        919.29|purchase|2024-04-03T12:59:42Z|\n",
      "|    1714766442|     65|          NULL|transfer|2024-05-03T13:00:42Z|\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data: \n",
    "jdbc_url = \"jdbc:postgresql://psql-postgres-1:5432/mydatabase\"\n",
    "table_name = \"transactions\"\n",
    "user = \"myuser\"\n",
    "password = \"mysecretpassword\"\n",
    "\n",
    "psql_df = spark_utils.get_psql_data(jdbc_url, table_name, user, password)\n",
    "psql_df.show()\n",
    "\n",
    "csv_file_path = 'problematic_transactions.csv'\n",
    "\n",
    "csv_df = spark_utils.get_csv_data(csv_file_path)\n",
    "csv_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Schema Valid: True\n",
      "CSV Schema Valid: True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, StringType\n",
    "\n",
    "# Define the expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),  # corrected \"user_d\" to \"user_id\"\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Function to validate schema (only compares column names)\n",
    "def validate_schema(df, expected_schema):\n",
    "    actual_columns = df.columns\n",
    "    expected_columns = [field.name for field in expected_schema]\n",
    "    \n",
    "    missing_columns = [col for col in expected_columns if col not in actual_columns]\n",
    "    extra_columns = [col for col in actual_columns if col not in expected_columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns: {missing_columns}\")\n",
    "    if extra_columns:\n",
    "        print(f\"Unexpected columns: {extra_columns}\")\n",
    "    \n",
    "    return not missing_columns and not extra_columns\n",
    "\n",
    "# Example usage\n",
    "print(\"Database Schema Valid:\", validate_schema(psql_df, expected_schema))\n",
    "print(\"CSV Schema Valid:\", validate_schema(csv_df, expected_schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null/Empty Values in Database Data:\n",
      "+--------------+-------+------+----+---------+\n",
      "|transaction_id|user_id|amount|type|timestamp|\n",
      "+--------------+-------+------+----+---------+\n",
      "|             0|      0|     0|   0|        0|\n",
      "+--------------+-------+------+----+---------+\n",
      "\n",
      "Null/Empty Values in CSV Data:\n",
      "+--------------+-------+------+----+---------+\n",
      "|transaction_id|user_id|amount|type|timestamp|\n",
      "+--------------+-------+------+----+---------+\n",
      "|             0|      5|     9|   0|        0|\n",
      "+--------------+-------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "def check_nulls(df):\n",
    "    \"\"\"\n",
    "    Checks for null or NaN values in a Spark DataFrame and prints the count of such values for each column.\n",
    "    :param df: Spark DataFrame to check for nulls\n",
    "    \"\"\"\n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_types = {\"double\", \"float\"}\n",
    "    numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in numeric_types]\n",
    "    \n",
    "    # Build the condition for each column\n",
    "    null_counts = df.select([\n",
    "        count(\n",
    "            when(\n",
    "                col(c).isNull() | (col(c).isNaN() if c in numeric_cols else col(c).isNull()),\n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    \n",
    "    # Show the results\n",
    "    null_counts.show()\n",
    "\n",
    "# Assuming psql_df and csv_df are your DataFrames loaded from PostgreSQL and CSV respectively\n",
    "print(\"Null/Empty Values in Database Data:\")\n",
    "check_nulls(psql_df)\n",
    "\n",
    "print(\"Null/Empty Values in CSV Data:\")\n",
    "check_nulls(csv_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Null/Missing data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Duplicate Check:\n",
      "Duplicate Records Found: 0\n",
      "CSV Duplicate Check:\n",
      "Duplicate Records Found: 30\n"
     ]
    }
   ],
   "source": [
    "# Check for Duplicates\n",
    "def check_duplicates(df, key_columns):\n",
    "    duplicate_count = df.groupBy(key_columns).count().filter(col(\"count\") > 1).count()\n",
    "    print(f\"Duplicate Records Found: {duplicate_count}\")\n",
    "\n",
    "print(\"Database Duplicate Check:\")\n",
    "check_duplicates(psql_df, [\"transaction_id\"])\n",
    "\n",
    "print(\"CSV Duplicate Check:\")\n",
    "check_duplicates(csv_df, [\"transaction_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated_df Schema Valid: True\n",
      "+--------------+-------+------+----+---------+\n",
      "|transaction_id|user_id|amount|type|timestamp|\n",
      "+--------------+-------+------+----+---------+\n",
      "|             0|      5|     9|   0|        0|\n",
      "+--------------+-------+------+----+---------+\n",
      "\n",
      "Duplicate Records Found: 12\n",
      "Duplicate Records Found: 0\n"
     ]
    }
   ],
   "source": [
    "# Combine data and run all tests\n",
    "\n",
    "aligned_csv_df = csv_df.select(psql_df.columns)\n",
    "concatenated_df = psql_df.union(aligned_csv_df)\n",
    "concatenated_df = concatenated_df.distinct()\n",
    "# print(concatenated_df.show())\n",
    "\n",
    "\n",
    "print(\"concatenated_df Schema Valid:\", validate_schema(concatenated_df, expected_schema))\n",
    "(check_nulls(concatenated_df))\n",
    "(check_duplicates(concatenated_df, [\"transaction_id\"]))\n",
    "\n",
    "# Handle duplicates\n",
    "deduplicated_df = concatenated_df.dropDuplicates([\"transaction_id\"])\n",
    "(check_duplicates(deduplicated_df, [\"transaction_id\"]))\n",
    "\n",
    "\n",
    "\n",
    "# Clean data\n",
    "\n",
    "# Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency Check Between Database and CSV:\n",
      "+--------------+-------+------+----+---------+-------+------+----+---------+\n",
      "|transaction_id|user_id|amount|type|timestamp|user_id|amount|type|timestamp|\n",
      "+--------------+-------+------+----+---------+-------+------+----+---------+\n",
      "+--------------+-------+------+----+---------+-------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consistency Check\n",
    "def compare_data(df1, df2, key_column):\n",
    "    joined_df = df1.join(df2, on=key_column, how=\"inner\")\n",
    "    mismatched_df = joined_df.filter(df1[\"amount\"] != df2[\"amount\"])\n",
    "    mismatched_df.show()\n",
    "\n",
    "print(\"Consistency Check Between Database and CSV:\")\n",
    "compare_data(psql_df, csv_df, \"transaction_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count in Database: 495\n",
      "Row Count in CSV: 129\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "|transaction_id|user_id|amount|    type|          timestamp|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "|    1704169733|      9|361.83|transfer|2024-01-01 20:28:53|\n",
      "|    1706848193|     77| 55.28|transfer|2024-02-01 20:29:53|\n",
      "|    1709353853|     69| 82.96|purchase|2024-03-01 20:30:53|\n",
      "|    1712028713|     88| 36.10|transfer|2024-04-01 20:31:53|\n",
      "|    1714620773|     22| 94.44|transfer|2024-05-01 20:32:53|\n",
      "+--------------+-------+------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "|transaction_id|user_id|        amount|    type|           timestamp|\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "|    1704314682|     45|960.52 dollars|  refund|2024-01-03T12:44:42Z|\n",
      "|    1706993142|     70|434.41 dollars|purchase|2024-02-03T12:45:42Z|\n",
      "|    1709498802|     80|856.68 dollars|  refund|2024-03-03T12:46:42Z|\n",
      "|    1712173662|     10|          NULL|transfer|2024-04-03T12:47:42Z|\n",
      "|    1714765722|     88|        108.19|transfer|2024-05-03T12:44:42Z|\n",
      "+--------------+-------+--------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Post-Ingestion\n",
    "print(\"Row Count in Database:\", psql_df.count())\n",
    "print(\"Row Count in CSV:\", csv_df.count())\n",
    "\n",
    "psql_df.show(5)\n",
    "csv_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+--------+\n",
      "|summary|     transaction_id|           user_id|            amount|    type|\n",
      "+-------+-------------------+------------------+------------------+--------+\n",
      "|  count|                495|               495|               495|     495|\n",
      "|   mean|1.718286145418182E9|51.183838383838385|        501.332970|    NULL|\n",
      "| stddev|  9198036.564008962| 28.80171886413176|277.83947109554646|    NULL|\n",
      "|    min|         1704169733|                 1|             10.71|purchase|\n",
      "|    max|         1733203479|               100|            997.41|transfer|\n",
      "+-------+-------------------+------------------+------------------+--------+\n",
      "\n",
      "+-------+--------------------+-----------------+------------------+--------+--------------------+\n",
      "|summary|      transaction_id|          user_id|            amount|    type|           timestamp|\n",
      "+-------+--------------------+-----------------+------------------+--------+--------------------+\n",
      "|  count|                 129|              124|               120|     129|                 129|\n",
      "|   mean|1.7183739406046512E9|53.79838709677419| 514.3769306930693|    NULL|                NULL|\n",
      "| stddev|   9055968.064134622|30.42302824808696|260.49590945250014|    NULL|                NULL|\n",
      "|    min|          1704314682|                2|            101.67|purchase|2024-01-03T12:18:42Z|\n",
      "|    max|          1733264382|              100|            998.21|transfer|2024/12-03T13:55:42Z|\n",
      "+-------+--------------------+-----------------+------------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psql_df.describe().show()\n",
    "csv_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Missing Columns: Transaction_date missing in CSV\n",
      "[LOG] Null Values: Null values in 'amount' column\n"
     ]
    }
   ],
   "source": [
    "def log_issues(issue_type, details):\n",
    "    print(f\"[LOG] {issue_type}: {details}\")\n",
    "\n",
    "log_issues(\"Missing Columns\", \"Transaction_date missing in CSV\")\n",
    "log_issues(\"Null Values\", \"Null values in 'amount' column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: great_expectations in /opt/conda/lib/python3.11/site-packages (1.2.5)\n",
      "Requirement already satisfied: altair<5.0.0,>=4.2.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.2.2)\n",
      "Requirement already satisfied: cryptography>=3.2 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (41.0.4)\n",
      "Requirement already satisfied: jinja2>=2.10 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.19.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.23.1)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from great_expectations) (23.2)\n",
      "Requirement already satisfied: posthog<3,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.5.0)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.10.3)\n",
      "Requirement already satisfied: pyparsing>=2.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.31.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.16 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (0.17.39)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (1.11.3)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (4.12.2)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (5.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (1.24.4)\n",
      "Requirement already satisfied: pandas<2.2,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from great_expectations) (2.0.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=2.10->great_expectations) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=2.5.1->great_expectations) (0.10.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog<3,>=2.1.0->great_expectations) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog<3,>=2.1.0->great_expectations) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from posthog<3,>=2.1.0->great_expectations) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.7->great_expectations) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.7)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/usr/local/spark/python/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'great_expectations.data_context' has no attribute 'DataContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m psql_df_pd \u001b[38;5;241m=\u001b[39m psql_df\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a new Expectations Suite\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataContext\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/great_expectations/directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m suite \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcreate_expectation_suite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add expectations\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Example: Expect column values to be in a set\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'great_expectations.data_context' has no attribute 'DataContext'"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "psql_df_pd = psql_df.toPandas()\n",
    "\n",
    "# Create a new Expectations Suite\n",
    "context = ge.data_context.DataContext(\"/path/to/great_expectations/directory\")\n",
    "suite = context.create_expectation_suite(\"my_suite\")\n",
    "\n",
    "# Add expectations\n",
    "# Example: Expect column values to be in a set\n",
    "batch = ge.dataset.PandasDataset(psql_df_pd)  # Wrap the pandas DataFrame with a batch\n",
    "batch.expect_column_values_to_be_in_set(\"column_name\", [\"value1\", \"value2\"])\n",
    "\n",
    "# Validate the dataset\n",
    "validation_results = batch.validate()\n",
    "print(validation_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
